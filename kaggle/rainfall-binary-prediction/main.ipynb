{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ab210-5069-4bc6-afdd-66ca0b9cf67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "Downloading playground-series-s5e3.zip to /Users/idb0123/Desktop/ia-learning/kaggle/rainfall-binary-prediction\n",
      "  0%|                                               | 0.00/59.0k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 59.0k/59.0k [00:00<00:00, 1.46MB/s]\n",
      "Archive:  playground-series-s5e3.zip\n",
      "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c playground-series-s5e3\n",
    "!unzip playground-series-s5e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd866a6-318e-46de-b46e-36994b6fa505",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf playground-series-s5e3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24958b-bfcc-478a-8df0-11ebd829fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a3b01-e944-4542-9e76-8b38c86191f5",
   "metadata": {},
   "source": [
    "## Competition\n",
    "\n",
    "Based on ROC, illustrates the performance of a Binary Classifier.\n",
    "\n",
    "From the statement, we know that this must be able to guess the probability. The first things that comes to my mind are:\n",
    "\n",
    "- KNN\n",
    "- Trees\n",
    "- Random Forest\n",
    "- SVMs\n",
    "- Logistic Regression\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d9369-3084-4568-b56f-37aec0c78a45",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5587e5-0972-47d1-b040-934d54bd257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b29915-b596-4f01-a666-cd472e6fb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94335c-d3c7-4241-ad2c-ae4bff36418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a290a99-6a73-4d9d-9f9f-cfca00430519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543263c7-f20d-442b-8bad-066219b56b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10045c80-bc20-4dc7-b703-c0aa60a02bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f39c7e-b5e0-491d-9e6b-3319e0c2cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d140d1-3aaf-4c1b-8130-e303b15886ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739e325-cf39-4480-81a6-8a22b2331c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61caff-53d7-48f9-9564-4421a2a9ba07",
   "metadata": {},
   "source": [
    "- **Rainfall**: although is a binary 0-1, we can see `mean` is not near 0.5. Then we have to take into account this for our model\n",
    "- **Reescaling is necessary**, as this is pure numerical value data.\n",
    "- `winddirection, humidity, cloud` have kinda unique values. This might led to categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6285503-e5ae-44b1-b3ee-157bf9ea1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = \"rainfall\"\n",
    "numerical_variables = list(filter(lambda e: e != target_variable, df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b485b-6c01-4b25-abfc-ad50c940cd9d",
   "metadata": {},
   "source": [
    "### Numerical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14a33a-7791-462d-9697-4f135939fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and display a row of plots for a single variable\n",
    "def create_variable_plots(df, variable):\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Box plot\n",
    "    sns.boxplot(x=df[variable], ax=axes[0])\n",
    "    axes[0].set_xlabel(variable)\n",
    "    axes[0].set_title(f\"Box Plot for {variable}\")\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(df[variable], kde=True, bins=30, ax=axes[1])\n",
    "    axes[1].set_xlabel(variable)\n",
    "    axes[1].set_ylabel(\"Frequency\")\n",
    "    axes[1].set_title(f\"Histogram for {variable}\")\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_variable_pie_plot(df, variable):\n",
    "    sns.set_style('whitegrid')\n",
    "    counts = df[variable].value_counts()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    ax.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90, \n",
    "           colors=sns.color_palette(\"pastel\"), wedgeprops={'edgecolor': 'black'})\n",
    "    \n",
    "    plt.title(f\"Pie chart of {variable}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0dbac-e5bd-4eee-bb71-22b717c832c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_var in numerical_variables:\n",
    "    create_variable_plots(df, num_var)\n",
    "\n",
    "create_variable_pie_plot(df, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d389f5-f3cb-4b62-8b70-ca423a56893d",
   "metadata": {},
   "source": [
    "Outliers are considered when the value is 1.5 bigger than the IQR (Q3 - Q1). We can see that: `pressure`, `mintemp` (just 1 outlier), `dewpoint`, `humidity`, `cloud` and `windspeed` have several outliers.\n",
    "\n",
    "A more extensive analysis from each variable:\n",
    "\n",
    "`pressure`: follows *almost* a Normal Distribution at around 1013 (from the boxplot) -> need to smooth values \n",
    "\n",
    "`maxtemp`: left skewed two peaks. Seasons(? -> binning (agrupar) into temperature ranges to capture seasonality.\n",
    "\n",
    "`temperature`: left skewed two peaks. Seasons(? -> need to do a relation with max_temp and min_temp\n",
    "\n",
    "`min_temp`: left sweked two peaks (much bigger the right one) -> as there are some outliers, we can try to smooth data\n",
    "\n",
    "`dewpoint`: left sweked. Lots of outliers -> log transformation for reducing outliers\n",
    "\n",
    "`humidity`: outliers. left skewed (most data >= 75%) -> what happens with low humidity?\n",
    "\n",
    "`cloud`: outliers. left skewed (most data >= 70%) -> correlation with humidity\n",
    "\n",
    "`sunshine`: highly right skewed. Near 0 -> see negative correlation with cloud and humidity\n",
    "\n",
    "`winddirection`: bimodal distribution. 50º and 200º. -> hot encoding for capturing possible patterns\n",
    "\n",
    "`windspeed`: right skewed. Some outliers -> log transformation\n",
    "\n",
    "\n",
    "Seems that `rainfall` is unbalanced, which will probably affect to the model performance.\n",
    "\n",
    "- Since the data is skewed toward rainy days, models may struggle to correctly predict non-rainy days.\n",
    "- We might need resampling techniques (e.g., SMOTE, undersampling) or adjusted class weights to improve balance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78c9f7-84a7-4922-ae2d-8a7ac14b67d4",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2c219-d511-49e8-b845-4d33951aa9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(df.select_dtypes(include=[\"int64\", \"float64\"]).corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e9f7b-5c70-4efe-9468-170329d68f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after, more easy to understand, as repeteated variables are not displayed\n",
    "variables = [col for col in df.columns if col in numerical_variables]\n",
    "\n",
    "# Adding variables to the existing list\n",
    "train_variables = variables \n",
    "\n",
    "# Calculate correlation matrices for train_data and test_data\n",
    "corr_train = df[train_variables].corr()\n",
    "\n",
    "\n",
    "# Create masks for the upper triangle\n",
    "mask_train = np.triu(np.ones_like(corr_train, dtype=bool))\n",
    "\n",
    "# Set the text size and rotation\n",
    "annot_kws = {\"size\": 8, \"rotation\": 45}\n",
    "\n",
    "# Generate heatmaps for train_data\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "ax_train = sns.heatmap(corr_train, mask=mask_train, cmap='viridis', annot=True,\n",
    "                      square=True, linewidths=.5, xticklabels=1, yticklabels=1, annot_kws=annot_kws)\n",
    "plt.title('Correlation Heatmap - Train Data')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c951f8-9d02-4c58-b48a-1f683459bd2d",
   "metadata": {},
   "source": [
    "We should also do a Pairplot usedo for discerning the `kernel density estimation`, also known as KDE, helps us to “smooth” and explore data that doesn't follow any typical probability density distribution, such as normal distribution, binomial distribution, etc.\n",
    "\n",
    "Most of our data don't follow a normal distr. then, its a good idea to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4651de9-73b5-4df2-9aa8-f6f341d26e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pairplot for train_data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.pairplot(df.drop(columns=['id'], errors='ignore'), diag_kind=\"kde\", corner=True)\n",
    "plt.suptitle(\"Pairplot - Train Data\", fontsize=40, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67ea30-71ac-451f-9bc1-2539c3b33b6e",
   "metadata": {},
   "source": [
    "What do we get from this? Tons of things:\n",
    "\n",
    "1. From the first graphics, we can see very high positive correlations (>0.8). This means that some data can be dropped and still not lose a lot of information. This happens specially with `maxTemp`, `pressure`, `minTemp` and `dewPoint` -> drop some data (feature selection)\n",
    "2. From the first graphics, we can see high correlations (0.5 around). `cloud`--`humidity`. `winddirection` -- `maxTemp`, `pressure`, `minTemp` and `dewPoint`. -> scale or transform data to improve the model\n",
    "3. From the first graphics, we can see high negative correlations. `cloud` -- `sunshine`. -> drop some data (feature selection)\n",
    "\n",
    "For correlations that are not so clear, more analysis should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599acb3d-1119-4c03-b25d-907245eeedb9",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Feature Engineering\n",
    "\n",
    "Remove noise and useless atributes, so it won't affect the model :)\n",
    "\n",
    "- Enhances patterns & relationships → Helps models capture hidden trends.\n",
    "- Removes noise & redundancy → Keeps only what truly matters.\n",
    "- Improves accuracy & efficiency → Optimized data leads to better predictions.\n",
    "- Reduces dimensionality → Fewer features = Faster computation\n",
    "- \n",
    "After creating new atributes, the idea is to compute an statistical analysis again and see if we can see new relations or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0c1e0-5624-439f-b444-14eacdd7cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f261a-816f-431b-af65-5f116ddc3bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['winddirection'] = df_test['winddirection'].fillna(df_test['winddirection'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06311e6e-6f9f-4af6-b12e-8c5989a31278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_feature_engineering(df):\n",
    "    # Temperature Difference - Difference between max and min temperature\n",
    "    df['Temp_Diff'] = df['maxtemp'] - df['mintemp']\n",
    "\n",
    "    # Dew Point Spread - Difference between temperature and dew point\n",
    "    df['Dew_Point_Spread'] = df['temparature'] - df['dewpoint']\n",
    "\n",
    "    # Humidity Category - Binning humidity into low, medium, and high\n",
    "    df['Humidity_Category'] = pd.cut(df['humidity'], bins=[0, 50, 80, 100], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "    # Cloud Cover Category - Grouping cloud cover into bins\n",
    "    df['Cloud_Cover_Category'] = pd.cut(df['cloud'], bins=[0, 30, 70, 100], labels=['Clear', 'Partly Cloudy', 'Overcast'])\n",
    "\n",
    "    # Sunshine Duration Category - Categorizing sunshine duration\n",
    "    df['Sunshine_Category'] = pd.cut(df['sunshine'], bins=[-1, 3, 7, 13], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "    # Wind Speed Intensity - Categorizing wind speeds\n",
    "    df['Wind_Speed_Intensity'] = pd.cut(df['windspeed'], bins=[0, 10, 25, 60], labels=['Calm', 'Breezy', 'Windy'])\n",
    "\n",
    "    # Wind Direction Grouping - Binning wind direction into 4 quadrants\n",
    "    df['Wind_Quadrant'] = pd.cut(df['winddirection'], bins=[0, 90, 180, 270, 360], labels=['NE', 'SE', 'SW', 'NW'], include_lowest=True)\n",
    "\n",
    "    # Interaction Feature: Pressure & Humidity - Multiply to capture pressure-humidity effects\n",
    "    df['Pressure_Humidity_Interaction'] = df['pressure'] * df['humidity']\n",
    "\n",
    "    # Interaction Feature: Wind & Cloud Cover - Wind effect on cloud cover\n",
    "    df['Wind_Cloud_Interaction'] = df['windspeed'] * df['cloud']\n",
    "\n",
    "    # Temperature Ratio - Normalized temperature based on max recorded value - the closer to 1.0, the hotter!\n",
    "    df['Temp_Ratio'] = df['temparature'] / df['maxtemp'].max()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to both train and test data\n",
    "df = perform_feature_engineering(df)\n",
    "df_test = perform_feature_engineering(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b056074-4e31-4728-82ad-f9acff2d8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8cbf1-1965-4b25-ad8c-c8fbb578d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3ccdd-7ae7-491c-8382-187224cd97d5",
   "metadata": {},
   "source": [
    "### Remove new outliers\n",
    "\n",
    "But new outliers could have appeared in the new columns... Why should we remove it? **The add noise to the data, reducing the precission of the model**.\n",
    "\n",
    "Obviously, we know which columns have to go through this part because of the previous graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18e3c0-e360-496b-9654-71de0208726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['dewpoint', 'humidity', 'cloud', 'windspeed', 'Temp_Diff', 'Dew_Point_Spread', 'Pressure_Humidity_Interaction', 'Wind_Cloud_Interaction', 'Temp_Ratio']\n",
    "\n",
    "# Function to remove outliers using IQR and visualize\n",
    "def remove_outliers_iqr_with_plot(data, column):\n",
    "    Q1 = data[column].quantile(0.15)\n",
    "    Q3 = data[column].quantile(0.85)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter the data\n",
    "    filtered_data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "    \n",
    "    # Calculate the number of rows deleted\n",
    "    rows_deleted = len(data) - len(filtered_data)\n",
    "    \n",
    "    # Plot the distribution with outliers\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=data[column], color='lightblue', flierprops={'marker': 'o', 'markersize': 5, 'markerfacecolor': 'red'})\n",
    "    \n",
    "    # Highlight Q1 and Q3\n",
    "    plt.axvline(Q1, color='green', linestyle='--', label='Q1 (10th Percentile)')\n",
    "    plt.axvline(Q3, color='blue', linestyle='--', label='Q3 (90th Percentile)')\n",
    "    \n",
    "    # Highlight lower and upper bounds\n",
    "    plt.axvline(lower_bound, color='red', linestyle='-', label='Lower Bound')\n",
    "    plt.axvline(upper_bound, color='red', linestyle='-', label='Upper Bound')\n",
    "\n",
    "    plt.title(f'Outlier Detection for {column}')\n",
    "    plt.legend()\n",
    "    plt.xlabel(column)\n",
    "    plt.show()\n",
    "    \n",
    "    return filtered_data, rows_deleted\n",
    "\n",
    "# Apply function to each numerical column and visualize\n",
    "rows_deleted_total = 0\n",
    "\n",
    "for column in columns_to_check:\n",
    "    df, rows_deleted = remove_outliers_iqr_with_plot(df, column)\n",
    "    rows_deleted_total += rows_deleted\n",
    "    print(f\"Rows deleted for {column}: {rows_deleted}\")\n",
    "\n",
    "print(f\"Total rows deleted: {rows_deleted_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2eb2e-b1c6-416b-830b-1fec8a32a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1810ca-c654-4b69-af2a-a8927ecf4997",
   "metadata": {},
   "source": [
    "### Transformation of distributions\n",
    "\n",
    "Almost all real data is skewed (does not follow a normal distr.). Thus, we must try to aproximate it to one. This can be use doing transformations.\n",
    "\n",
    "The skew-umbral depends (here it was 0.7). For ML, it might be interesting to use one that is lower.\n",
    "\n",
    "**THIS IS USUALLY DONE FOR TRAINING TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a5ba3-1a90-4a8f-b5f6-1e3b1461bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [FOR TRAIN]\n",
    "# Identify features with skewness greater than 0.75\n",
    "skewed_features = df[numerical_variables].skew()[df[numerical_variables].skew() > 0.70].index.values\n",
    "\n",
    "# Print the list of variables to be transformed\n",
    "print(\"Features to be transformed (skewness > 0.75):\")\n",
    "display(skewed_features)\n",
    "\n",
    "# Plot skewed features before transformation\n",
    "for feature in skewed_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[feature], bins=50, kde=True, color='blue')\n",
    "    plt.title(f'Distribution of {feature} before log transformation')\n",
    "    plt.show()\n",
    "\n",
    "# Apply log1p (log transform) transformation to skewed features\n",
    "df[skewed_features] = np.log1p(df[skewed_features])\n",
    "\n",
    "# Plot skewed features after transformation\n",
    "for feature in skewed_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[feature], bins=50, kde=True, color='green')\n",
    "    plt.title(f'Distribution of {feature} after log transformation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55b8bc-21c5-4101-8eb1-b1bda489f7e9",
   "metadata": {},
   "source": [
    "### Feature Encoding\n",
    "\n",
    "As always KNN (for instance), can't process categorical variables. We must hot-encode, dummy-encode or whatever in order to be able to process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe792c0-2bbe-4eb0-bf98-eb2e4698b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = df.select_dtypes(include=['category']).columns\n",
    "for column in category_columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac64de-6afa-476f-a790-825064b3a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoding = df[category_columns]\n",
    "df_test_encoding = df_test[category_columns]\n",
    "\n",
    "# Dropping selected columns for scaling - the ones that are not categorical\n",
    "# (numerical) must be scaled for better model performance\n",
    "df_to_scale = df.drop(category_columns, axis=1)\n",
    "df_test_to_scale = df_test.drop(category_columns, axis=1)\n",
    "\n",
    "df_encoded = pd.get_dummies(df_encoding, columns=category_columns, drop_first=True)\n",
    "df_test_encoded = pd.get_dummies(df_test_encoding, columns=category_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaadff4b-5abc-472b-860a-a42b82bd647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1bfbf-c606-4b87-8ec8-ff1119e6eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcd0be-338e-4f6d-960e-b8b498acbbd6",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "There are tons of scalers, the most common ones are:\n",
    "- MinMax()\n",
    "- Estandarization\n",
    "- Robust Scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a851048-39fb-49f7-bd30-8e06fba8d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# this could have be done with a Pipeline() if we wanted to try several scalers\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "minmax_scaler.fit(df_to_scale.drop(['rainfall'], axis=1))\n",
    "\n",
    "# Scale the training data\n",
    "scaled_data_train = minmax_scaler.transform(df_to_scale.drop(['rainfall'], axis=1))\n",
    "scaled_train_df = pd.DataFrame(scaled_data_train, columns=df_to_scale.drop(['rainfall'], axis=1).columns)\n",
    "\n",
    "# Scale the test data using the parameters from the training data\n",
    "scaled_data_test = minmax_scaler.transform(df_test_to_scale)\n",
    "scaled_test_df = pd.DataFrame(scaled_data_test, columns=df_test_to_scale.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165f031-1520-458e-9513-ca4f321c4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81fd47-73ab-43af-85f4-5f3841f9cf98",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Wopsie, model time. After data processing, I would like to try the following models:\n",
    "- KNN\n",
    "- Trees\n",
    "- Forest\n",
    "- XgBoost - want to try this\n",
    "- SVMs\n",
    "- Logistic Regression\n",
    "- Genetic Programming - want to try this\n",
    "\n",
    "So we will create a model for each and see what we obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19454cb0-f2bf-4e45-a2ce-d15ff3718f83",
   "metadata": {},
   "source": [
    "### KNN Model with Cross-validation\n",
    "\n",
    "Data split will follow my beloved 60-20-20... Wait. We can't, as we already have the Train and Test dataset.\n",
    "But we can do is adecuate the train dataset for having a training and validating subset, then testing them with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8a994-6a3f-43cf-88b8-6c6d1685dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013d60b-8aea-4d10-a6f6-cffa842eea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0256f8-6d28-4944-b193-83aca759d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = KFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "best_acc = None\n",
    "best_model = None\n",
    "scores = []\n",
    "\n",
    "for neigh in range(2, 20, 2):\n",
    "    model = KNeighborsClassifier(n_neighbors=neigh, weights=\"distance\") \n",
    "    fold_scores = []  # Almacenar los scores de los 5 folds\n",
    "\n",
    "    for train_idx, val_idx in kv.split(scaled_data_train):\n",
    "        X_train, X_val = scaled_train_df.iloc[train_idx], scaled_train_df.iloc[val_idx]\n",
    "        y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predecir en validación\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Evaluar el modelo\n",
    "        score = roc_auc_score(y_val, y_pred)\n",
    "        fold_scores.append(score)  # Guardamos el score de cada fold\n",
    "    \n",
    "    mean_score = np.mean(fold_scores)  # Promediamos los 5 folds\n",
    "\n",
    "    if best_acc is None or mean_score > best_acc:\n",
    "        best_acc = mean_score\n",
    "        best_model = model\n",
    "        \n",
    "    scores.append(mean_score)  # Guardamos solo el promedio de los folds\n",
    "\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(len(scores))  # Ahora será 10 (uno por cada valor de `neigh`)\n",
    "print(f\"Mean CV score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4e606-2d3e-48ec-9548-fef0c718a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best model is {best_model} with an accuracy of {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da0865-8d4c-4516-997c-383925aa1348",
   "metadata": {},
   "source": [
    "This precissions was done cross-eval method. However, we must execute it for the official data to test and upload it to see the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a36a5-77ea-43f8-b6b5-150395b5ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(scaled_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add839a7-a6a6-43b4-9a65-44254f9d5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b8486-6596-4336-b69a-289dd2468c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ebd6b-cf92-4066-adce-98940350c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],\n",
    "    \"rainfall\":y_pred \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44071ee-5d9b-426f-950a-2527c2ea6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6f293-c225-4fba-b5c5-b6cadb9b234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e90ff0d-6efa-442a-9c4b-f4002e0bec9e",
   "metadata": {},
   "source": [
    "After updating it to Kaggle, we got a score of **0.68812**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e536a-02b5-4d7a-917a-b9228daeafc6",
   "metadata": {},
   "source": [
    "### Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa1426-59d9-45df-9a10-dd91e2f367fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909dc375-6cbf-44ab-b6a8-a31f67998e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_acc = 0.0\n",
    "crit = [\"gini\", \"entropy\"]\n",
    "\n",
    "for max_d in range(2, 20, 1): # max_depth = aprox log(n_rows)\n",
    "    for min_s_s in range(2, 50, 1):\n",
    "        for c in crit:\n",
    "            model = DecisionTreeClassifier(criterion=c, max_depth=max_d, min_samples_leaf=min_s_s, random_state=random_seed)\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "            score = scores.mean() # average of all cross-val\n",
    "\n",
    "            if score > best_acc:\n",
    "                best_acc = score\n",
    "                best_model = model\n",
    "\n",
    "print(f\"Best model: {best_model} with precission {best_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a596648-2146-4653-ba21-d31f01e730c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(scaled_test_df)\n",
    "y_pred.shape\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],\n",
    "    \"rainfall\": y_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc472d-3b92-45a8-bda4-413d198f0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c7aa9-7499-479e-b4e4-dcd8cbf1511e",
   "metadata": {},
   "source": [
    "Those were tons of iterations. But we did have what it was promissed: the model! With a Kaggle precission of: **0.77648**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60a74f-b675-44cb-a2b3-d8a0c377642c",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
